install.packages("forecast")
library(forecast)
plot(forecast(model))
forecast(model)
forecast(model)
plot(forecast(model))
forecast(model)
a<-sarima.for(log(sales),12,1,1,0,0,1,1,12)
plot.ts(c(sales,exp(a$pred)), main='Monthly sales + Forecast', ylab='', col='blue', lwd=3)
#The input predictor variables can be categorical and/or numeric variables.
# Load the data and remove NAs
data("PimaIndiansDiabetes2", package = "mlbench")
#Naive Bayes Classifier
#Naive Bayes classifier predicts the class membership probability of observations using Bayes theorem, which is based on conditional probability, that is the probability of something to happen, given that something else has already occurred.
#Observations are assigned to the class with the largest probability score.
library(tidyverse)  #tidyverse for easy data manipulation and visualization
library(caret)      #caret for easy machine learning workflow
#Naive Bayes Classifier
#Naive Bayes classifier predicts the class membership probability of observations using Bayes theorem, which is based on conditional probability, that is the probability of something to happen, given that something else has already occurred.
#Observations are assigned to the class with the largest probability score.
install.packages("caret")
library(caret)      #caret for easy machine learning workflow
#The input predictor variables can be categorical and/or numeric variables.
# Load the data and remove NAs
data("PimaIndiansDiabetes2", package = "mlbench")
#Naive Bayes Classifier
#Naive Bayes classifier predicts the class membership probability of observations using Bayes theorem, which is based on conditional probability, that is the probability of something to happen, given that something else has already occurred.
#Observations are assigned to the class with the largest probability score.
install.packages("mlbench")
library(caret)      #caret for easy machine learning workflow
#The input predictor variables can be categorical and/or numeric variables.
# Load the data and remove NAs
data("PimaIndiansDiabetes2", package = "mlbench")
View(PimaIndiansDiabetes2)
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
#-----------Computing Naive Bayes
library("klaR")
#-----------Computing Naive Bayes
install.packages("klaR")
library("klaR")
# Fit the model
model <- NaiveBayes(diabetes ~., data = train.data)
summary(model)
# Make predictions
predictions <- model %>% predict(test.data)
# Fit the model
model <- NaiveBayes(diabetes ~., data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model accuracy
mean(predictions$class == test.data$diabetes)
#----------Using caret R package
library(klaR)
# Build the model
set.seed(123)
model <- train(diabetes ~., data = train.data, method = "nb",
trControl = trainControl("cv", number = 10))
# Make predictions
predicted.classes <- model %>% predict(test.data)
# Model n accuracy
mean(predicted.classes == test.data$diabetes)
#-----------------------------------
#This is also known as shrinkage or regularization methods.
#The consequence of imposing this penalty, is to reduce (i.e. shrink) the coefficient values towards zero. This allows the less contributive variables to have a coefficient close to zero or equal zero.
library(tidyverse)  #tidyverse for easy data manipulation and visualization
library(caret)      #caret for easy machine learning workflow
library(glmnet)     #glmnet, for computing penalized regression
install.packages("glmnet")
library(glmnet)     #glmnet, for computing penalized regression
# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
createDataPartition(p = 0.8, list = FALSE)
View(Boston)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]
#-----Additional data preparation
#y for storing the outcome variable
#x for holding the predictor variables. This should be created using the function model.matrix() allowing to automatically transform any qualitative variables (if any) into dummy variables
#     (Chapter @ref(regression-with-categorical-variables)), which is important because glmnet() can only take numerical, quantitative inputs. After creating the model matrix, we remove the intercept component at index = 1.
# Predictor variables
x <- model.matrix(medv~., train.data)[,-1]
# Outcome variable
y <- train.data$medv
View(Boston)
#----------r function
#We'll use the R function glmnet() [glmnet package] for computing penalized linear regression models.
glmnet(x, y, alpha = 1, lambda = NULL)
#x: matrix of predictor variables
#y: the response or outcome variable, which is a binary variable.
#alpha: the elasticnet mixing parameter. Allowed values include:
#"1": for lasso regression
#"0": for ridge regression
#a value between 0 and 1 (say 0.3) for elastic net regression.
#lamba: a numeric value defining the amount of shrinkage. Should be specify by analyst.
#you need to specify a constant lambda to adjust the amount of the coefficient shrinkage. The best lambda for your data, can be defined as the lambda that minimize the cross-validation prediction error rate. This can be determined automatically using the function cv.glmnet().
#The best model is defined as the model that has the lowest prediction error, RMSE (Chapter @ref(regression-model-accuracy-metrics)).
# -----Find the best lambda using cross-validation
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0)
# Display the best lambda value
cv$lambda.min
#----------r function
#We'll use the R function glmnet() [glmnet package] for computing penalized linear regression models.
glmnet(x, y, alpha = 1, lambda = NULL)
#x: matrix of predictor variables
#y: the response or outcome variable, which is a binary variable.
#alpha: the elasticnet mixing parameter. Allowed values include:
#"1": for lasso regression
#"0": for ridge regression
#a value between 0 and 1 (say 0.3) for elastic net regression.
#lamba: a numeric value defining the amount of shrinkage. Should be specify by analyst.
#you need to specify a constant lambda to adjust the amount of the coefficient shrinkage. The best lambda for your data, can be defined as the lambda that minimize the cross-validation prediction error rate. This can be determined automatically using the function cv.glmnet().
#The best model is defined as the model that has the lowest prediction error, RMSE (Chapter @ref(regression-model-accuracy-metrics)).
# -----Find the best lambda using cross-validation
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0)
# Display the best lambda value
cv$lambda.min
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
# Display regression coefficients
coef(model)
# Make predictions on the test data
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
RMSE = RMSE(predictions, test.data$medv),
Rsquare = R2(predictions, test.data$medv)
)
#The only difference between the R code used for ridge regression is that, for lasso regression you need to specify the argument alpha = 1 instead of alpha = 0 (for ridge regression).
# Find the best lambda using cross-validation
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 1)
# Display the best lambda value
cv$lambda.min
# Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
# Dsiplay regression coefficients
coef(model)
# Make predictions on the test data
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()
# Model performance metrics
data.frame(
RMSE = RMSE(predictions, test.data$medv),
Rsquare = R2(predictions, test.data$medv)
)
#---------------------------------------elastic net regession----------------------------
#Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).
#The elastic net regression can be easily computed using the caret workflow, which invokes the glmnet package.
#We use caret to automatically select the best tuning parameters alpha and lambda. The caret packages tests a range of possible alpha and lambda values, then selects the best values for lambda and alpha, resulting to a final model that is an elastic net model.
#Here, we'll test the combination of 10 different values for alpha and lambda. This is specified using the option tuneLength.
#The best alpha and lambda values are those values that minimize the cross-validation error
# Build the model using the training set
set.seed(123)
model <- train(
medv ~., data = train.data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Best tuning parameter
model$bestTune
# Coefficient of the final model. You need to specify the best lambda
coef(model$finalModel, model$bestTune$lambda)
# Make predictions on the test data
x.test <- model.matrix(medv ~., test.data)[,-1]
predictions <- model %>% predict(x.test)
# Model performance metrics
data.frame(
RMSE = RMSE(predictions, test.data$medv),
Rsquare = R2(predictions, test.data$medv)
)
#All things equal, we should go for the simpler model. In our example, we can choose the lasso or the elastic net regression models.
#============================================================
#-----------------------Using caret package------------------------------
#caret will automatically choose the best tuning parameter values, compute the final model and evaluate the model performance using cross-validation techniques.
#-------Setup a grid range of lambda values::
lambda <- 10^seq(-3, 3, length = 100)
lambda
#---------Compute ridge regression::
# Build the model
set.seed(123)
ridge <- train(
medv ~., data = train.data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 0, lambda = lambda)
)
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- ridge %>% predict(test.data)
# Model prediction performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
Rsquare = R2(predictions, test.data$medv)
)
#----------Compute lasso regression:
# Build the model
set.seed(123)
lasso <- train(
medv ~., data = train.data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(test.data)
# Model prediction performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
Rsquare = R2(predictions, test.data$medv)
)
#-----------Elastic net regression:
# Build the model
set.seed(123)
elastic <- train(
medv ~., data = train.data, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Model coefficients
coef(elastic$finalModel, elastic$bestTune$lambda)
# Make predictions
predictions <- elastic %>% predict(test.data)
# Model prediction performance
data.frame(
RMSE = RMSE(predictions, test.data$medv),
Rsquare = R2(predictions, test.data$medv)
)
#------------Comparing models performance:
#The performance of the different models - ridge, lasso and elastic net - can be easily compared using caret. The best model is defined as the one that minimizes the prediction error.
models <- list(ridge = ridge, lasso = lasso, elastic = elastic)
resamples(models) %>% summary( metric = "RMSE")
#Cross-Validation
#http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/
#R2, RMSE and MAE are used to measure the regression model performance during cross-validation.
#--------------------------------------
#The basic idea, behind cross-validation techniques, consists of dividing the data into two sets:
#The training set, used to train (i.e. build) the model;
#and the testing set (or validation set), used to test (i.e. validate) the model by estimating the prediction error.
library(tidyverse)  #tidyverse for easy data manipulation and visualization
library(caret)      #caret for easy machine learning workflow
# Load the data
data("swiss")
# Inspect the data
sample_n(swiss, 3)
#------------------------
#cross-validation algorithms can be summarized as follow:
#Reserve a small sample of the data set
#Build (or train) the model using the remaining part of the data set
#Test the effectiveness of the model on the the reserved sample of the data set. If the model works well on the test data set, then it's good.
#--------------------------
# Split the data into training and test set
set.seed(123)
training.samples <- swiss$Fertility %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- swiss[training.samples, ]
test.data <- swiss[-training.samples, ]
# Build the model
model <- lm(Fertility ~., data = train.data)
# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(test.data)
data.frame( R2 = R2(predictions, test.data$Fertility),
RMSE = RMSE(predictions, test.data$Fertility),
MAE = MAE(predictions, test.data$Fertility))
#When comparing two models, the one that produces the lowest test sample RMSE is the preferred model.
#Note that, the validation set method is only useful when you have a large data set that can be partitioned.
RMSE(predictions, test.data$Fertility)/mean(test.data$Fertility)
#Dividing the RMSE by the average value of the outcome variable will give you the prediction error rate, which should be as small as possible:
#-------------------------Leave one out cross validation - LOOCV----------------------
#Leave out one data point and build the model on the rest of the data set
#Test the model against the data point that is left out at step 1 and record the test error associated with the prediction
#Repeat the process for all data points
#Compute the overall prediction error by taking the average of all these test error estimates recorded at step 2.
# Define training control
train.control <- trainControl(method = "LOOCV")
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
trControl = train.control)
# Summarize the results
print(model)
# Define training control
set.seed(123)
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
trControl = train.control)
# Summarize the results
print(model)
#------------------------------Repeated K-fold cross-validation--------------------------
#The process of splitting the data into k-folds can be repeated a number of times, this is called repeated k-fold cross validation.
# Define training control
set.seed(123)
train.control <- trainControl(method = "repeatedcv",
number = 10, repeats = 3)
# Train the model
model <- train(Fertility ~., data = swiss, method = "lm",
trControl = train.control)
# Summarize the results
print(model)
View(lasso)
library(precrec)
install.packages("precrec")
library(precrec)
# Load a test dataset
data(P10N10)
# Load a test dataset
data(P10N10)
P10N10
view(P10N10)
sample_n(P10N10)
# Calculate ROC and Precision-Recall curves
sscurves <- evalmod(scores = P10N10$scores, labels = P10N10$labels)
# The ggplot2 package is required
library(ggplot2)
# Show ROC and Precision-Recall plots
autoplot(sscurves)
sample_n(as.matrix(P10N10))
sample_n  <-(as.matrix(P10N10))
View(sample_n)
#Evaluation of Classification Model Accuracy
#After building a predictive classification model, you need to evaluate the performance of the model, that is how good the model is in predicting the outcome of new observations test data that have been not used to train the model.
#we know the actual outcome of observations in the test data set, the performance of the predictive model can be assessed by comparing the predicted outcome values against the known outcome values.
library(tidyverse)
library(caret)
#-----Building a classification model
#To keep things simple, we’ll perform a binary classification, where the outcome variable can have only two possible values: negative vs positive.
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
pima.data <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(pima.data, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- pima.data$diabetes %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- pima.data[training.samples, ]
test.data <- pima.data[-training.samples, ]
library(MASS)
# Fit LDA
fit <- lda(diabetes ~., data = train.data)
# Make predictions on the test data
predictions <- predict(fit, test.data)
prediction.probabilities <- predictions$posterior[,2]
predicted.classes <- predictions$class
observed.classes <- test.data$diabetes
#-------Overall classification accuracy
#Determining the raw classification accuracy is the first step in assessing the performance of a model.
#classification error rate is defined as the proportion of observations that have been misclassified. Error rate = 1 - accuracy
#The raw classification accuracy and error can be easily computed by comparing the observed classes in the test data against the predicted classes by the model:
accuracy <- mean(observed.classes == predicted.classes)
accuracy
error <- mean(observed.classes != predicted.classes)
error
#--------Confusion matrix
#The R function table() can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.
# It compares the observed and the predicted outcome values and shows the number of correct and incorrect predictions categorized by type of outcome.
# Confusion matrix, number of cases
table(observed.classes, predicted.classes)
# Confusion matrix, proportion of cases
prop.table() %>% round(digits = 3
# Confusion matrix, proportion of cases
table(observed.classes, predicted.classes) %>%
# Confusion matrix, proportion of cases
table(observed.classes, predicted.classes) %>%
# Confusion matrix, proportion of cases
table(observed.classes, predicted.classes) %>%
prop.table() %>% round(digits = 3)
# Confusion matrix, proportion of cases
table(observed.classes, predicted.classes) %>%
prop.table() %>% round(digits = 3)
#Technically the raw prediction accuracy of the model is defined as (TruePositives + TrueNegatives)/SampleSize.
#-------------Precision, Recall and Specificity
#Precision::, which is the proportion of true positives among all the individuals that have been predicted to be diabetes-positive by the model. This represents the accuracy of a predicted positive outcome. Precision = TruePositives/(TruePositives + FalsePositives).
#Sensitivity (or Recall)::, which is the True Positive Rate (TPR) or the proportion of identified positives among the diabetes-positive population (class = 1). Sensitivity = TruePositives/(TruePositives + FalseNegatives).
#Specificity::, which measures the True Negative Rate (TNR), that is the proportion of identified negatives among the diabetes-negative population (class = 0). Specificity = TrueNegatives/(TrueNegatives + FalseNegatives).
#False Positive Rate (FPR)::, which represents the proportion of identified positives among the healthy individuals (i.e. diabetes-negative). This can be seen as a false alarm. The FPR can be also calculated as 1-specificity. When positives are rare, the FPR can be high, leading to the situation where a predicted positive is most likely a negative.
#Sensitivy and Specificity are commonly used to measure the performance of a predictive model.
confusionMatrix(predicted.classes, observed.classes,
positive = "pos")#you might need to specify the optional argument positive, which is a character string for the factor level that corresponds to a “positive” result (if that makes sense for your data).
#ROC curve
#The ROC curve (or receiver operating characteristics curve ) is a popular graphical measure for assessing the performance or the accuracy of a classifier, which corresponds to the total proportion of correctly classified observations.
#Since we don’t usually know the probability cutoff in advance, the ROC curve is typically used to plot the true positive rate (or sensitivity on y-axis) against the false positive rate (or “1-specificity” on x-axis) at all possible probability cutoffs.
# This shows the trade off between the rate at which you can correctly predict something with the rate of incorrectly predicting something. Another visual representation of the ROC plot is to simply display the sensitive against the specificity.
#The Area Under the Curve (AUC) summarizes the overall performance of the classifier, over all possible probability cutoffs. It represents the ability of a classification algorithm to distinguish 1s from 0s (i.e, events from non-events or positives from negatives).
#For a good model, the ROC curve should rise steeply, indicating that the true positive rate (y-axis) increases faster than the false positive rate (x-axis) as the probability threshold decreases.
#So, the “ideal point” is the top left corner of the graph, that is a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that the larger the AUC the better the classifier.
#The AUC metric varies between 0.50 (random classifier) and 1.00. Values above 0.80 is an indication of a good classifier.
#-----Computing and plotting ROC curve
library(pROC)
#ROC curve
#The ROC curve (or receiver operating characteristics curve ) is a popular graphical measure for assessing the performance or the accuracy of a classifier, which corresponds to the total proportion of correctly classified observations.
#Since we don’t usually know the probability cutoff in advance, the ROC curve is typically used to plot the true positive rate (or sensitivity on y-axis) against the false positive rate (or “1-specificity” on x-axis) at all possible probability cutoffs.
# This shows the trade off between the rate at which you can correctly predict something with the rate of incorrectly predicting something. Another visual representation of the ROC plot is to simply display the sensitive against the specificity.
#The Area Under the Curve (AUC) summarizes the overall performance of the classifier, over all possible probability cutoffs. It represents the ability of a classification algorithm to distinguish 1s from 0s (i.e, events from non-events or positives from negatives).
#For a good model, the ROC curve should rise steeply, indicating that the true positive rate (y-axis) increases faster than the false positive rate (x-axis) as the probability threshold decreases.
#So, the “ideal point” is the top left corner of the graph, that is a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that the larger the AUC the better the classifier.
#The AUC metric varies between 0.50 (random classifier) and 1.00. Values above 0.80 is an indication of a good classifier.
#-----Computing and plotting ROC curve
install.packages("pROC")
library(pROC)
# Compute roc
res.roc <- roc(observed.classes, prediction.probabilities)
plot.roc(res.roc, print.auc = TRUE)
#The gray diagonal line represents a classifier no better than random chance.
#A highly performant classifier will have an ROC that rises steeply to the top-left corner, that is it will correctly identify lots of positives without misclassifying lots of negatives as positives.
#In our example, the AUC is 0.85, which is close to the maximum ( max = 1). So, our classifier can be considered as very good. A classifier that performs no better than chance is expected to have an AUC of 0.5 when evaluated on an independent test set not used to train the model.
# Extract some interesting results
roc.data <- data_frame(
thresholds = res.roc$thresholds,
sensitivity = res.roc$sensitivities,
specificity = res.roc$specificities
)
# Get the probality threshold for specificity = 0.6
roc.data %>% filter(specificity >= 0.6)
#The best threshold with the highest sum sensitivity + specificity can be printed as follow. There might be more than one threshold.
plot.roc(res.roc, print.auc = TRUE, print.thres = "best")
#result:::Here, the best probability cutoff is 0.335 resulting to a predictive classifier with a specificity of 0.84 and a sensitivity of 0.660.
#Note that, print.thres can be also a numeric vector containing a direct definition of the thresholds to display:
plot.roc(res.roc, print.thres = c(0.3, 0.5, 0.7))
#-------Multiple ROC curves
#If you have grouping variables in your data, you might wish to create multiple ROC curves on the same plot.
# Create some grouping variable
glucose <- ifelse(test.data$glucose < 127.5, "glu.low", "glu.high")
age <- ifelse(test.data$age < 28.5, "young", "old")
roc.data <- roc.data %>%
filter(thresholds !=-Inf) %>%
mutate(glucose = glucose, age =  age)
# Create ROC curve
ggplot(roc.data, aes(specificity, sensitivity)) +
geom_path(aes(color = age))+
scale_x_reverse(expand = c(0,0))+
scale_y_continuous(expand = c(0,0))+
geom_abline(intercept = 1, slope = 1, linetype = "dashed")+
theme_bw()
#------------Multiclass settings
#We start by building a linear discriminant model using the iris data set, which contains the length and width of sepals and petals for three iris species.
# We want to predict the species based on the sepal and petal parameters using LDA.
# Load the data
data("iris")
# Split the data into training (80%) and test set (20%)
set.seed(123)
training.samples <- iris$Species %>%
createDataPartition(p = 0.8, list = FALSE)
train.data <- iris[training.samples, ]
test.data <- iris[-training.samples, ]
# Build the model on the train set
library(MASS)
model <- lda(Species ~., data = train.data)
# Model accuracy
confusionMatrix(predictions$class, test.data$Species)
#Performance metrics (sensitivity, specificity, …) of the predictive model can be calculated, separately for each class, comparing each factor level to the remaining levels (i.e. a “one versus all” approach).
# Make predictions on the test data
predictions <- model %>% predict(test.data)
# Model accuracy
confusionMatrix(predictions$class, test.data$Species)
# it provides several features which makes it a one stop solution for all the modeling needs for supervised machine learning problems.
install.packages("caret", dependencies = c("Depends", "Suggests"))
setwd("C:/Users/Ami Laddani/Desktop/MACHINE LEARNING_COURSEERA")
library(readxl)
USDA <- read.csv("USDA.csv")
View(USDA)
summary(USDA)
which.max(USDS$Sodium)
which.max(USDA$Sodium)
names(USDA)
USDA$Description[265]
HighSodium = subset(USDA, Sodium>10000)
nrow(HighSodium)
HighSodium$Description
match("CAVIAR" , USDA$Description)
USDA$Sodium[4154]
summary(USDA$Sodium)
sd(USDA$Sodium)
sd(USDA$Sodium, na.rm = TRUE)
library(readxl)
train <- read.tsv("C:/Users/Ami Laddani/Desktop/Movie Review Sentiment Analysis_data/train.tsv")
View(train)
library(readxl)
train <- read_tsv("C:/Users/Ami Laddani/Desktop/Movie Review Sentiment Analysis_data/train.tsv")
View(train)
library(tidyverse)
train <- read_tsv("C:/Users/Ami Laddani/Desktop/Movie Review Sentiment Analysis_data/train.tsv")
View(train)
library(tidyverse)
test <- read_tsv("C:/Users/Ami Laddani/Desktop/Movie Review Sentiment Analysis_data/test.tsv")
View(test)
library(tidytext) # working with text
library(wordcloud) # visualising text
library(gridExtra) # extra plot options
library(grid) # extra plot options
library(keras) # deep learning with keras
install.packages("keras")
library(keras) # deep learning with keras
View(test)
head(train)
library(readxl)
train <- read.csv("C:/Users/Ami Laddani/Desktop/Store Item Demand Forecasting Challenge/train.csv")
View(train)
